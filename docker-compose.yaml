version: '3.8'
services:
  minio:
    image: minio/minio:RELEASE.2025-02-03T21-03-04Z-cpuv1
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - datalake_internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    env_file:
      - .env
    restart: always

  zookeeper:
    image: zookeeper:3.9.2
    hostname: zookeeper
    environment:
      ZOO_MY_ID: 1
      ZOO_CLIENT_PORT: 2181
      ZOO_4LW_COMMANDS_WHITELIST: ruok,srvr,stat,conf
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog
    networks:
      - datalake_internal
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 5s
      timeout: 5s
      retries: 10
    restart: always

  kafka1:
    build:
      context: .
      dockerfile: Dockerfile.kafka
    hostname: kafka1
    ports:
      - "9091:9091"
    environment:
      - KAFKA_ADVERTISED_LISTENERS=LISTENER_DOCKER_INTERNAL://kafka1:19091,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9091
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=LISTENER_DOCKER_INTERNAL
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_BROKER_ID=1
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_OPTS=-javaagent:/jmx_exporter/jmx_prometheus_javaagent-0.19.0.jar=7075:/jmx_exporter/kafka-2_0_0.yml
    volumes:
      - kafka1_data:/var/lib/kafka/data
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - datalake_internal
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7075/metrics || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 180s
    restart: always

  kafka-init-topics:
    build:
      context: .
      dockerfile: Dockerfile.kafka    
    hostname: kafka-init-topics
    user: root                       
    command: > 
      bash -c '
        export KAFKA_LOG4J_OPTS="-Dlog4j.configuration=file:/etc/kafka/log4j.properties";
        echo "Waiting for Kafka to be ready..." ;
        for i in {1..120}; do
          nc -z kafka1 19091 && break;
          sleep 1;
        done ;
        if ! nc -z kafka1 19091; then
          echo "Kafka não está pronto."; exit 1;
        fi ;
        echo "Creating Kafka topic: api-topic" ;
        kafka-topics --create --if-not-exists \
          --bootstrap-server kafka1:19091 \
          --topic api-topic \
          --partitions 1 \
          --replication-factor 1 \
          --config retention.ms=3600000
      '
    depends_on:
      kafka1:
        condition: service_healthy
    networks:
      - datalake_internal
    volumes:
      - /tmp:/tmp       
    restart: always             

  kafdrop:
    image: obsidiandynamics/kafdrop:3.31.0
    restart: "always"
    ports:
      - "9002:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka1:19091
    depends_on:
      kafka1:
        condition: service_healthy
    networks:
      - datalake_internal

  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: meu-spark:com-curl
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_WEBUI_PORT=8088
      - SPARK_EXTRA_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,io.delta:delta-core_2.12:2.4.0
    ports:
      - "7077:7077"
      - "8088:8088"
    volumes:
      - spark_data:/data
    networks:
      - datalake_internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  spark-worker-1:
    image: meu-spark:com-curl
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=8g
      - SPARK_WORKER_CORES=2
      - SPARK_EXTRA_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,io.delta:delta-core_2.12:2.4.0
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8081:8081"
    volumes:
      - spark_data:/data
    networks:
      - datalake_internal

  spark-worker-2:
    image: meu-spark:com-curl
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=8g
      - SPARK_WORKER_CORES=2
      - SPARK_EXTRA_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,io.delta:delta-core_2.12:2.4.0
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8181:8081"
    volumes:
      - spark_data:/data
    networks:
      - datalake_internal

  jupyter-pyspark:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    container_name: jupyter-pyspark
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - PYSPARK_SUBMIT_ARGS=--packages io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell
      - PYTHONPATH=/opt/airflow:/opt/airflow/src
    env_file:
      - .env
    volumes:
      - notebooks:/home/jovyan/work
    networks:
      - datalake_internal
    restart: always

  airflow-postgres:
    image: postgres:13
    expose:
      - "5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    networks:
      - datalake_internal

  redis:
    image: redis:7.2.0
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - datalake_internal

  statsd-exporter:
    image: prom/statsd-exporter:v0.26.0
    container_name: statsd-exporter
    ports:
      - "8125:8125/udp"
      - "9102:9102"
    networks:
      - datalake_internal
    command:
      - "--statsd.listen-udp=:8125"
      - "--web.listen-address=:9102"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9102/metrics"]
      interval: 10s
      timeout: 5s
      retries: 3

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow 
    depends_on:
      airflow-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      statsd-exporter:
        condition: service_started
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=airflow
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=statsd-exporter
      - AIRFLOW__METRICS__STATSD_PORT=8125
      - AIRFLOW__METRICS__STATSD_PREFIX=airflow
      - PYTHONPATH=/opt/airflow:/opt/airflow/src
    env_file:
      - .env
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
      - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      - datalake_internal

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      statsd-exporter:
        condition: service_started
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=statsd-exporter
      - AIRFLOW__METRICS__STATSD_PORT=8125
      - AIRFLOW__METRICS__STATSD_PREFIX=airflow
      - PYTHONPATH=/opt/airflow:/opt/airflow/src
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
      - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
    command: scheduler
    networks:
      - datalake_internal

  airflow-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      statsd-exporter:
        condition: service_started
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - SPARK_EXTRA_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,io.delta:delta-core_2.12:2.4.0
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=statsd-exporter
      - AIRFLOW__METRICS__STATSD_PORT=8125
      - AIRFLOW__METRICS__STATSD_PREFIX=airflow
      - PYTHONPATH=/opt/airflow:/opt/airflow/src
    env_file:
      - .env
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
      - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
      - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
      - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
      - ${AIRFLOW_PROJ_DIR:-.}/src:/opt/airflow/src
    command: celery worker
    networks:
      - datalake_internal

  streamlit-app:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    restart: "always"
    ports:
      - "8501:8501"
    env_file:
      - .env
    volumes:
      - .:/app
    networks:
      - datalake_internal

  prometheus:
    image: prom/prometheus:v3.5.0
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - datalake_internal

  grafana:
    image: grafana/grafana:12.0.2
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - datalake_internal


networks:
  datalake_internal:
    driver: bridge

volumes:
  minio_data:
  airflow-postgres-db-volume:
  zookeeper_data:
  zookeeper_datalog:
  kafka1_data:
  spark_data:
  notebooks:
  grafana_data: